{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NwULeKAQxzqx",
    "outputId": "a60455b6-b1d8-4867-a3a6-d64f4a070066",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão dopandas: 1.0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Main:     \n",
    "    def show_current_time(self):\n",
    "        ini = time.time()\n",
    "        tz_SP = pytz.timezone('America/Sao_Paulo') \n",
    "        now = datetime.now(tz_SP)\n",
    "        current_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        print(\"Current Time =\", current_time)         \n",
    "\n",
    "base_path = 'D:/'\n",
    "#base_path = '/content/drive/My Drive/Colab Notebooks/'\n",
    "\n",
    "print(\"Versão dopandas: \" + pd.__version__)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\draus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1166, 100000)\n",
      "(1166,)\n"
     ]
    }
   ],
   "source": [
    "# roda na linha de comando : python -m spacy download pt_core_news_lg\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "all_stopwords_spacy = nlp.Defaults.stop_words\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('portuguese') + list(punctuation) + list(all_stopwords_spacy)\n",
    "           + list(['ão','õe','ões','art','fls','vol','fl','janeiro','fevereiro','março','abril','maio','junho','julho',\n",
    "                 'agosto','setembro','outubro','novembro','dezembro','josé','jose','maria',\n",
    "                   'ii','iii','iv','vi','vii','viii','xix','xi','xii','xiii','xiv','xv','xvi','xvii','xviii','xix',\n",
    "                  'xx']))\n",
    "\n",
    "\n",
    "os.chdir(base_path+'/dou')\n",
    "df_original = pd.read_pickle(\"dataframe_original_balanceado.pkl\")\n",
    "max_features = 100000\n",
    "\n",
    "corpus = df_original['texto']\n",
    "y = df_original['target'].to_numpy()\n",
    "y = y.astype('int')\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(analyzer='word', ngram_range=(1, 2),max_df= 0.75,\n",
    "                            max_features= max_features, stop_words=frozenset(stops),\n",
    "                                           token_pattern=r'[a-zA-Z\\u00C0-\\u00FF]{3,100}')),\n",
    "                  ('tfid', TfidfTransformer(norm= 'l2', use_idf= True))]).fit(corpus)\n",
    "X = pipe.fit_transform(corpus).toarray()\n",
    "\n",
    "#print(pipe['count'].transform(corpus))\n",
    "#print(pipe['tfid'].idf_)\n",
    "#print(pipe['count'].get_feature_names())\n",
    "#print(pipe['count'].vocabulary_['presidente'])\n",
    "#print(pipe['tfid'].idf_[pipe['count'].vocabulary_['unidade']])\n",
    "\n",
    "#df_idf = pd.DataFrame()\n",
    "#for key, value in pipe['count'].vocabulary_.items():\n",
    "    #print(key,value)\n",
    "    #print(pipe['tfid'].idf_[value]\n",
    "#    df_idf[key] = [pipe['tfid'].idf_[value]]\n",
    "#print(X[0:])\n",
    "#print(df_idf.head)\n",
    "#print(stops)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#df_idf.to_pickle(\"dataframe_idf_treina.pkl\")\n",
    " \n",
    "outfile = open('pipeline_treina.pkl','wb')\n",
    "pickle.dump(pipe,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando variável categórica \"tipo\" (OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1166, 100000)\n",
      "(1166,)\n",
      "(1166, 100152)\n"
     ]
    }
   ],
   "source": [
    "hot_enc = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
    "X_words = X\n",
    "X_tipo = hot_enc.fit_transform(df_original[['_tipo']])\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X = np.concatenate((X_words,X_tipo),axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# save to csv file\n",
    "#savetxt('X.csv', X, delimiter=',')\n",
    "#savetxt('y.csv', y, delimiter=',')\n",
    "outfile = open('X.pkl','wb')\n",
    "pickle.dump(X,outfile)\n",
    "outfile.close()\n",
    "outfile = open('y.pkl','wb')\n",
    "pickle.dump(y,outfile)\n",
    "outfile.close()\n",
    "outfile = open('hot_encoder_treina.pkl','wb')\n",
    "pickle.dump(hot_enc,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 100000)\n",
      "(644,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.chdir(base_path+'/dou')\n",
    "df_original = pd.read_pickle(\"dataframe_original_desbalanceado.pkl\")\n",
    "\n",
    "corpus = df_original['texto']\n",
    "y = df_original['target'].to_numpy()\n",
    "y = y.astype('int')\n",
    "\n",
    "X = pipe.transform(corpus).toarray()\n",
    "\n",
    "#print(pipe['count'].transform(corpus).toarray())\n",
    "#print(pipe['tfid'].idf_)\n",
    "#print(X[0:])\n",
    "#print(stops)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando variável categórica \"tipo\" (OneHotEncoder) - Classes desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 100000)\n",
      "(644, 152)\n",
      "(644,)\n",
      "(644, 100152)\n"
     ]
    }
   ],
   "source": [
    "X_words = X\n",
    "X_tipo = hot_enc.fit_transform(df_original[['_tipo']])\n",
    "\n",
    "print(X.shape)\n",
    "print(X_tipo.shape)\n",
    "print(y.shape)\n",
    "X = np.concatenate((X_words,X_tipo),axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "outfile = open('X_d.pkl','wb')\n",
    "pickle.dump(X,outfile)\n",
    "outfile.close()\n",
    "outfile = open('y_d.pkl','wb')\n",
    "pickle.dump(y,outfile)\n",
    "outfile.close()\n",
    "# save to csv file\n",
    "#savetxt('X_d.csv', X, delimiter=',')\n",
    "#savetxt('y_d.csv', y, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "FonteDeDados.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
